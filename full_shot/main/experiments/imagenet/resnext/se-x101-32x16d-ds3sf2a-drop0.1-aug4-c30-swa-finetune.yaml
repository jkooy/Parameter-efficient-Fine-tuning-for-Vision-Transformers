BASE: ['se-x101-32x16d-ds3sf2a-drop0.1-aug4-w5c120-bnwd0-larc.yaml']
SWA:
  ENABLED: True
  BEGIN_EPOCH: 0
MODEL:
  PRETRAINED: 'DATASET/models/IN-1k/se-x101-32x16d-ds3sf2a-drop0.1-aug4-w5c120-bnwd0-larc/final_state.pth'
  PRETRAINED_LAYERS: ['*']
TRAIN:
  END_EPOCH: 30
  BATCH_SIZE_PER_GPU: 64
  LARC: False
  LR_SCHEDULER:
    METHOD: CyclicLR
    BASE_LR: 0.0000025
    MAX_LR: 0.00025
